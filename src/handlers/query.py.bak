from js import Response
import json
import uuid

async def handle_query(request, env):
    """ Endpoint principal para consultas RAG """

    # Parse do JSON
    body = await request.json()
    query = body.get("query")
    session_id = body.get("session_id")

    # Validação
    if not query:
        return Response.json(
            {"error": "Missing query"}, 
            status=400
        )

    # Gerar Embedding da pergunta do usuário
    query_embedding_response = await env.AI.run(
        '@cf/qwen/qwen3-embedding-0.6b',
        {"text": query}
    )

    query_embedding = query_embedding_response["data"][0]

    # Buscar chunks mais relevantes no Vectorize
    search_results = await env.VECTORIZE_INDEX.query(
        query_embedding,
        {"topK": 5}
    )
    
    # Extrair informações dos resultados
    relevant_chunks = [
        {
            "id": match["id"],
            "score": match["score"],
            "metadata": match["metadata"],
        }
        for match in search_results["matches"]
    ]
    
    # Construir contexto a partir dos chunks relevantes
    context_text = "\n\n---\n\n".join([
        chunk["metadata"]["text_preview"] 
        for chunk in relevant_chunks
    ])

    # Prompt para o modelo
    system_prompt = f"""
    Você é um assistente especializado em regulação dos setores de Gás e Energia.
    Responda às perguntas com base EXCLUSIVAMENTE no contexto fornecido abaixo.
    Se a informação não estiver no contexto, diga que não encontrou a informação nos documentos disponíveis.
    Cite sempre as fontes (documento, artigo, cláusula) ao responder.
    
    Contexto:
    {context_text}"""
    
    # Montar mensagens para o LLM
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]

    # Chamar Llama 3.1 8B para gerar a resposta
    llm_response = await env.AI.run(
        '@cf/llama/llama3.1-8b-instruct-fast',
        {
            "messages": messages,
            "max_tokens": 512,
            "temperature": 0.7
        }
    )

    answer = llm_response["response"]

    # Gerar session_id se não for fornecido
    if not session_id:
        session_id = str(uuid.uuid4())

    # Preparar resposta
    response = {
        "answer": answer,
        "sources": [
            {
                "document_id": chunk["metadata"]["document_id"],
                "title": chunk["metadata"]["title"],
                "type": chunk["metadata"]["type"],
                "sector": chunk["metadata"]["sector"],
                "relevance_score": chunk["score"],
            }
            for chunk in relevant_chunks
        ],
        "session_id": session_id,
    }
    return Response.json(response)