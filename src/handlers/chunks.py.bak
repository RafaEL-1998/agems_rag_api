"""
Chunker Hier√°rquico e Sem√¢ntico para Documentos Regulat√≥rios
Otimizado para Resolu√ß√µes ANEEL e textos legais brasileiros

Este √© o m√≥dulo central de processamento documental da AGEMS.
Consolida extra√ß√£o de PDF, an√°lise sem√¢ntica e segmenta√ß√£o hier√°rquica.
"""

import re
import json
import io
import uuid
import os
import sys
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
import pdfplumber

# ================================================================================
# 1. AN√ÅLISE SEM√ÇNTICA & AUXILIARES
# ================================================================================

class AnalisadorRegulatorio:
    """Classe para an√°lise e extra√ß√£o de informa√ß√µes regulat√≥rias sem√¢nticas"""
    
    @staticmethod
    def extrair_referencias_cruzadas(texto: str) -> List[str]:
        """Extrai refer√™ncias cruzadas a outros artigos, par√°grafos ou leis"""
        referencias = []
        padroes = [
            r'art(?:igo)?\.?\s+(\d+(?:-[A-Z])?)[¬∫o¬∞]?',
            r'¬ß\s*(\d+(?:-[A-Z])?)[¬∫o¬∞]?',
            r'inciso\s+([IVXLCDM]+(?:-[A-Z])?)',
            r'Lei\s+n[¬∫o¬∞]?\s*[\d.]+/\d{4}',
            r'Decreto\s+n[¬∫o¬∞]?\s*[\d.]+/\d{4}',
            r'Resolu√ß√£o\s+(?:Normativa\s+)?n[¬∫o¬∞]?\s*[\d.]+/\d{4}',
        ]
        for padrao in padroes:
            matches = re.finditer(padrao, texto, re.IGNORECASE)
            referencias.extend([m.group(0) for m in matches])
        return list(set(referencias))
    
    @staticmethod
    def extrair_valores_numericos(texto: str) -> Dict[str, List]:
        """Extrai valores num√©ricos importantes (pot√™ncias, prazos, multas, porcentagens)"""
        valores = {
            'potencias_mw': [],
            'porcentagens': [],
            'prazos_dias': [],
            'valores_monetarios': []
        }
        
        # Pot√™ncias em MW/kW
        potencias = re.finditer(r'(\d+(?:[.,]\d+)?)\s*(?:MW|kW)', texto, re.IGNORECASE)
        valores['potencias_mw'] = [m.group(0) for m in potencias]
        
        # Porcentagens
        porcentagens = re.finditer(r'(\d+(?:[.,]\d+)?)\s*%', texto)
        valores['porcentagens'] = [m.group(0) for m in porcentagens]
        
        # Prazos em dias/meses
        prazos = re.finditer(r'(\d+)\s*(?:dias|meses)', texto, re.IGNORECASE)
        valores['prazos_dias'] = [m.group(0) for m in prazos]
        
        # Valores monet√°rios
        monetarios = re.finditer(r'R\$\s*[\d.,]+', texto)
        valores['valores_monetarios'] = [m.group(0) for m in monetarios]
        
        return valores
    
    @staticmethod
    def identificar_obrigacoes(texto: str) -> List[str]:
        """Identifica verbos e locu√ß√µes que indicam obrigatoriedade"""
        obrigacoes = []
        padroes = [
            r'dever√°\s+[^.;]+',
            r'√©\s+obrigat[o√≥]rio\s+[^.;]+',
            r'deve\s+[^.;]+',
            r'fica\s+obrigad[oa]\s+[^.;]+',
        ]
        for padrao in padroes:
            matches = re.finditer(padrao, texto, re.IGNORECASE)
            obrigacoes.extend([m.group(0) for m in matches])
        return obrigacoes
    
    @staticmethod
    def identificar_vedacoes(texto: str) -> List[str]:
        """Identifica proibi√ß√µes e veda√ß√µes"""
        vedacoes = []
        padroes = [
            r'n√£o\s+poder[√°a]\s+[^.;]+',
            r'√©\s+vedado\s+[^.;]+',
            r'fica\s+proibid[oa]\s+[^.;]+',
            r'n√£o\s+ser[√°a]\s+permitido\s+[^.;]+',
        ]
        for padrao in padroes:
            matches = re.finditer(padrao, texto, re.IGNORECASE)
            vedacoes.extend([m.group(0) for m in matches])
        return vedacoes

class OtimizadorConsultas:
    """Otimizador para expans√£o de termos de busca em regula√ß√£o"""
    @staticmethod
    def expandir_query(query: str) -> List[str]:
        queries = [query]
        sinonimos = {
            'autoriza√ß√£o': ['outorga', 'permiss√£o', 'concess√£o'],
            'empreendimento': ['projeto', 'instala√ß√£o', 'usina'],
            'pot√™ncia': ['capacidade instalada', 'gera√ß√£o'],
            'consumidor': ['usu√°rio', 'titular'],
            'faturamento': ['cobran√ßa', 'conta de luz']
        }
        query_lower = query.lower()
        for termo, s_list in sinonimos.items():
            if termo in query_lower:
                for s in s_list:
                    queries.append(query_lower.replace(termo, s))
        return queries

# ================================================================================
# 2. EXTRA√á√ÉO DE PDF (PDFPLUMBER)
# ================================================================================

async def extract_text_from_pdf(env, r2_key, start_page=1, limit_pages=100):
    """
    Extrai texto de um PDF armazenado no R2 usando pdfplumber.
    Injeta marcadores de p√°gina para preserva√ß√£o de metadados.
    """
    # 1. Recuperar objeto do R2
    obj = await env.agems_docs.get(r2_key)
    if not obj:
        raise Exception(f"Arquivo n√£o encontrado: {r2_key}")

    # 2. Converter para bytes e buffer de mem√≥ria
    from js import Uint8Array
    pdf_bytes = bytes(Uint8Array.new(await obj.arrayBuffer()))
    
    texto_acumulado = []
    
    # 3. Usar pdfplumber para extra√ß√£o rica
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        total_pages = len(pdf.pages)
        end_page = min(start_page + limit_pages - 1, total_pages)
        
        for i in range(start_page - 1, end_page):
            page = pdf.pages[i]
            text = page.extract_text(x_tolerance=2, y_tolerance=3, layout=True)
            if text:
                texto_acumulado.append(f"[[PAGINA:{i+1}]]\n{text}")
                
    return "\n\n".join(texto_acumulado), total_pages

# ================================================================================
# 3. SEGMENTA√á√ÉO HIER√ÅRQUICA (CHUNKING)
# ================================================================================

class TipoElemento(Enum):
    PREAMBULO = "preambulo"
    RESOLUCAO = "resolucao"
    TITULO = "titulo"
    CAPITULO = "capitulo"
    SECAO = "secao"
    SUBSECAO = "subsecao"
    ARTIGO = "artigo"
    PARAGRAFO = "paragrafo"
    INCISO = "inciso"
    ALINEA = "alinea"
    ITEM = "item"
    ANEXO = "anexo"

@dataclass
class ElementoRegulatorio:
    tipo: TipoElemento
    numero: str
    texto: str
    nivel: int
    pagina: int = 1
    revogado: bool = False
    pai: Optional['ElementoRegulatorio'] = None
    contexto_hierarquico: str = ""

class ChunkerRegulatorio:
    def __init__(self, tamanho_max_chunk: int = 1000, overlap_chars: int = 200, manter_contexto: bool = True):
        self.tamanho_max_chunk = tamanho_max_chunk
        self.overlap_chars = overlap_chars
        self.manter_contexto = manter_contexto
        self.analisador = AnalisadorRegulatorio()
        
        self.padroes = {
            'resolucao': re.compile(r'^RESOLU√á√ÉO\s+(?:NORMATIV\s*A\s+)?N[¬∫o¬∞]?\s*(\d+(?:[.,]\d+)?)[,/]\s*DE\s+\d+', re.IGNORECASE | re.MULTILINE),
            'titulo': re.compile(r'^T√çTULO\s+([IVXLCDM]+|[0-9]+|(?:DAS|DOS|DA|DO)\s+.+?)\s*(?:[-‚Äì‚Äî]\s*)?(.+)?$', re.IGNORECASE | re.MULTILINE),
            'capitulo': re.compile(r'^CAP√çTULO\s+([IVXLCDM]+|[0-9]+)\s*(?:[-‚Äì‚Äî]\s*)?(.+)?$', re.IGNORECASE | re.MULTILINE),
            'secao': re.compile(r'^SE√á√ÉO\s+([IVXLCDM]+|[0-9]+)\s*(?:[-‚Äì‚Äî]\s*)?(.+)?$', re.IGNORECASE | re.MULTILINE),
            'subsecao': re.compile(r'^SUBSE√á√ÉO\s+([IVXLCDM]+|[0-9]+)\s*(?:[-‚Äì‚Äî]\s*)?(.+)?$', re.IGNORECASE | re.MULTILINE),
            'artigo': re.compile(r'^Art\.?\s+(\d+(?:-[A-Z])?)[¬∫o¬∞]?\s*[-‚Äì‚Äî]?\s*(.+?)$', re.MULTILINE),
            'paragrafo': re.compile(r'^¬ß\s*(\d+(?:-[A-Z])?)[¬∫o¬∞]?\s*[-‚Äì‚Äî]?\s*(.+?)$', re.MULTILINE),
            'paragrafo_unico': re.compile(r'^Par√°grafo\s+√∫nico\.?\s*[-‚Äì‚Äî]?\s*(.+?)$', re.IGNORECASE | re.MULTILINE),
            'inciso': re.compile(r'^([IVXLCDM]+(?:-[A-Z])?)\s*[-‚Äì‚Äî]\s*(.+?)$', re.MULTILINE),
            'alinea': re.compile(r'^([a-z])\)\s*(.+?)$', re.MULTILINE),
            'item': re.compile(r'^\s*(\d+)\.(?!\d)\s+(.+?)$', re.MULTILINE),
            'anexo': re.compile(r'^ANEXO\s+([IVXLCDM]+|[0-9]+|[A-Z])\s*[-‚Äì‚Äî]?\s*(.+?)$', re.IGNORECASE | re.MULTILINE),
            'revogado': re.compile(r'\((?:revogad[oa]|suprimid[oa]|exclu[√≠i]d[oa]|eliminad[oa])\)', re.IGNORECASE),
            'marcador_pagina': re.compile(r'\[\[PAGINA:(\d+)\]\]', re.MULTILINE),
            'trash': [
                r'Este texto n√£o substitui o publicado no Di√°rio Oficial.*',
                r'Publicado no DOU de \d{2}/\d{2}/\d{2}.*',
                r'Di√°rio Oficial da Uni√£o\s?-\s?Se√ß√£o\s?\d+.*',
                r'Page \d+ of \d+.*',
                r'^\s*\d+\s*$',
                r'\d{2}/\d{2}/\d{2},\s\d{2}:\d{2}'
            ]
        }

    def limpar_texto(self, texto: str) -> str:
        for p in self.padroes['trash']:
            texto = re.sub(p, '', texto, flags=re.IGNORECASE | re.MULTILINE)
        
        char_fixes = {
            'T√ÉTULO': 'T√çTULO', 'CAP√ÉTULO': 'CAP√çTULO', '√É¬ß√É¬£o': '√ß√£o', '√É¬ß√É¬µes': '√ß√µes',
            '√É¬°': '√°', '√É¬©': '√©', '√É¬≠': '√≠', '√É¬≥': '√≥', '√É¬∫': '√∫', '√É': '√†', '√É¬™': '√™', 
            '√É¬¥': '√¥', '√É¬ß': '√ß', '√†': '√£'
        }
        for b, f in char_fixes.items(): texto = texto.replace(b, f)
        return texto.strip()

    def _normalizar_estrutura(self, texto: str) -> str:
        padroes = [r'(?<!^)(T√çTULO\s+)', r'(?<!^)(CAP√çTULO\s+)', r'(?<!^)(SE√á√ÉO\s+)', r'(?<!^)(SUBSE√á√ÉO\s+)', r'(?<!^)(ANEXO\s+)']
        for p in padroes: texto = re.sub(p, r'\n\1', texto, flags=re.MULTILINE)
        return re.sub(r'([a-z0-9√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß);:])\s+(Art\.?\s*\d+)', r'\1\n\2', texto)

    def parse_documento(self, texto: str) -> List[ElementoRegulatorio]:
        texto = self.limpar_texto(texto)
        texto = self._normalizar_estrutura(texto)
        elementos = []
        linhas = texto.split('\n')
        
        elemento_atual = None
        texto_acumulado = []
        pagina_atual = 1
        
        # Rastreio de sequ√™ncia para evitar falsos positivos (refer√™ncias cruzadas)
        # Formato: {contexto_pai: {tipo: ultimo_numero}}
        sequencias = {}

        for linha in linhas:
            linha = linha.strip()
            if not linha: continue
            
            if m_pag := self.padroes['marcador_pagina'].match(linha):
                pagina_atual = int(m_pag.group(1))
                continue

            # Detec√ß√£o de Elemento com Valida√ß√£o de Sequ√™ncia
            elemento_id = None
            
            # Auxiliar para validar se o n√∫mero faz sentido na sequ√™ncia atual
            def validar_seq(tipo: TipoElemento, num_str: str, ctx_pai: str) -> bool:
                if num_str == "√∫nico": return True
                try: 
                    # Extrai apenas o n√∫mero principal (ex: 1-A -> 1)
                    num = int(re.findall(r'\d+', num_str)[0])
                    key = f"{ctx_pai}|{tipo.value}"
                    ult = sequencias.get(key, 0)
                    
                    # Se for o primeiro, ou for o pr√≥ximo (n+1), ou se estivermos em modo "compila√ß√£o" (aceita repeti√ß√£o imediata para troca de reda√ß√£o)
                    if num == 1 or num == ult + 1 or num == ult:
                        sequencias[key] = num
                        return True
                    return False
                except: return True # Se n√£o for num√©rico puro, aceita por seguran√ßa

            # Tenta encontrar match com os padr√µes
            if m := self.padroes['resolucao'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.RESOLUCAO, m.group(1), linha, 0, pagina_atual)
            elif m := self.padroes['titulo'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.TITULO, m.group(1), linha, 1, pagina_atual)
            elif m := self.padroes['capitulo'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.CAPITULO, m.group(1), linha, 2, pagina_atual)
            elif m := self.padroes['secao'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.SECAO, m.group(1), linha, 3, pagina_atual)
            elif m := self.padroes['artigo'].match(linha):
                if validar_seq(TipoElemento.ARTIGO, m.group(1), "doc"):
                    elemento_id = ElementoRegulatorio(TipoElemento.ARTIGO, m.group(1), linha, 5, pagina_atual)
            elif m := self.padroes['paragrafo_unico'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.PARAGRAFO, "√∫nico", linha, 6, pagina_atual)
            elif m := self.padroes['paragrafo'].match(linha):
                ctx = elemento_atual.contexto_hierarquico if elemento_atual else ""
                # Se a frase termina com "da", "do", "ao", "do art", √© 100% refer√™ncia
                if not re.search(r'(?:o|a|ao|do|da|no|na|pelo|pela|dos|das|art\.?|caput)\s+$', '\n'.join(texto_acumulado)[-30:], re.I):
                    if validar_seq(TipoElemento.PARAGRAFO, m.group(1), ctx):
                        elemento_id = ElementoRegulatorio(TipoElemento.PARAGRAFO, m.group(1), linha, 6, pagina_atual)
            elif m := self.padroes['inciso'].match(linha):
                ctx = elemento_atual.contexto_hierarquico if elemento_atual else ""
                if validar_seq(TipoElemento.INCISO, m.group(1), ctx):
                    elemento_id = ElementoRegulatorio(TipoElemento.INCISO, m.group(1), linha, 7, pagina_atual)
            elif m := self.padroes['alinea'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.ALINEA, m.group(1), linha, 8, pagina_atual)
            elif m := self.padroes['item'].match(linha):
                elemento_id = ElementoRegulatorio(TipoElemento.ITEM, m.group(1), linha, 9, pagina_atual)

            if elemento_id:
                if elemento_atual: 
                    elemento_atual.texto = '\n'.join(texto_acumulado)
                    elementos.append(elemento_atual)
                else: 
                    # Pre√¢mbulo inicial
                    if texto_acumulado:
                        text_pre = '\n'.join(texto_acumulado)
                        elementos.append(ElementoRegulatorio(TipoElemento.PREAMBULO, "0", text_pre, 0, 1, contexto_hierarquico="Pre√¢mbulo"))
                
                elemento_atual = elemento_id
                texto_acumulado = [linha]
                # Atualizar hierarquia tempor√°ria para o pr√≥ximo validar_seq
                self._estabelecer_hierarquia(elementos + [elemento_atual])
            else:
                texto_acumulado.append(linha)
        
        if elemento_atual:
            elemento_atual.texto = '\n'.join(texto_acumulado)
            elementos.append(elemento_atual)
            
        self._estabelecer_hierarquia(elementos)
        return self._deduplicar_elementos(elementos)

    def _deduplicar_elementos(self, elementos: List[ElementoRegulatorio]) -> List[ElementoRegulatorio]:
        """
        Deduplica√ß√£o Inteligente:
        - Prioriza vers√µes com "(Reda√ß√£o dada por...)" ou "(Inclu√≠do por...)".
        - Em caso de mesma hierarquia, mant√©m a vers√£o mais recente e completa.
        """
        mapa_final = {}
        for e in elementos:
            if e.tipo == TipoElemento.PREAMBULO: continue
            chave = f"{e.tipo.value}|{e.numero}|{e.contexto_hierarquico}"
            
            if chave not in mapa_final:
                mapa_final[chave] = e
            else:
                # Heur√≠stica de substitui√ß√£o
                existente = mapa_final[chave]
                nova_redacao = any(x in e.texto.lower() for x in ["reda√ß√£o dada", "inclu√≠do pela", "inclu√≠da pela"])
                antiga_revogada = any(x in existente.texto.lower() for x in ["revogado", "suprimido", "exclu√≠do"])
                
                # Substitui se a nova for explicitamente uma altera√ß√£o ou se a antiga sumiu/√© menor
                if nova_redacao or antiga_revogada or len(e.texto) > len(existente.texto) * 1.5:
                    mapa_final[chave] = e
        
        # Preserva a ordem original de apari√ß√£o da chave
        final_list = []
        vistos = set()
        for e in elementos:
            chave = f"{e.tipo.value}|{e.numero}|{e.contexto_hierarquico}" if e.tipo != TipoElemento.PREAMBULO else "PREAMBULO"
            if chave == "PREAMBULO":
                final_list.append(e)
                continue
            if chave not in vistos:
                final_list.append(mapa_final[chave])
                vistos.add(chave)
        return final_list

    def _estabelecer_hierarquia(self, elementos: List[ElementoRegulatorio]):
        pilha = []
        nomes = {TipoElemento.TITULO: "T√≠tulo", TipoElemento.CAPITULO: "Cap√≠tulo", TipoElemento.SECAO: "Se√ß√£o", 
                 TipoElemento.ARTIGO: "Artigo", TipoElemento.PARAGRAFO: "Par√°grafo", TipoElemento.INCISO: "Inciso", TipoElemento.ALINEA: "Al√≠nea"}
        
        for e in elementos:
            while pilha and pilha[-1].nivel >= e.nivel: pilha.pop()
            if pilha: e.pai = pilha[-1]
            
            # Contexto
            parts = []
            curr = e
            while curr and curr.pai:
                curr = curr.pai
                if curr.tipo in nomes:
                    prefix = f"{nomes[curr.tipo]} {curr.numero}"
                    if curr.tipo == TipoElemento.PARAGRAFO: prefix = f"¬ß {curr.numero}" if curr.numero != "√∫nico" else "Par√°grafo √önico"
                    elif curr.tipo == TipoElemento.INCISO: prefix = f"Inc. {curr.numero}"
                    parts.insert(0, prefix)
            e.contexto_hierarquico = " > ".join(parts)
            pilha.append(e)

    def criar_chunks(self, texto: str) -> List[Dict]:
        elementos = [e for e in self.parse_documento(texto) if not self.padroes['revogado'].search(e.texto)]
        chunks = []
        chunk_id = 0
        
        for e in elementos:
            # Marcadores estruturais (T√≠tulo/Cap√≠tulo/Se√ß√£o) mesmo se curtos
            if e.tipo in [TipoElemento.TITULO, TipoElemento.CAPITULO, TipoElemento.SECAO] and len(e.texto) < 200:
                chunks.append(self._formatar_chunk(f"MARCADOR_ESTRUTURAL: {e.texto}", e, chunk_id))
                chunk_id += 1
                continue
            
            if e.tipo == TipoElemento.SUBSECAO and len(e.texto) < 100: continue

            if len(e.texto) <= self.tamanho_max_chunk:
                chunks.append(self._formatar_chunk(e.texto, e, chunk_id))
                chunk_id += 1
            else:
                for sub in self._dividir_texto(e):
                    chunks.append(self._formatar_chunk(sub, e, chunk_id, parte=True))
                    chunk_id += 1
        return chunks

    def _formatar_chunk(self, texto: str, e: ElementoRegulatorio, cid: int, parte: bool = False) -> Dict:
        prefix = f"[{e.contexto_hierarquico}] " if e.contexto_hierarquico else ""
        return {
            'chunk_id': f"chunk_{cid}",
            'texto': f"{prefix}{texto}",
            'tipo': e.tipo.value,
            'numero': e.numero,
            'nivel': e.nivel,
            'contexto_hierarquico': e.contexto_hierarquico,
            'pagina': e.pagina,
            'tamanho': len(texto),
            'parte_de_elemento_maior': parte,
            'semantica': {
                'obrigacoes': self.analisador.identificar_obrigacoes(texto),
                'vedacoes': self.analisador.identificar_vedacoes(texto),
                'referencias': self.analisador.extrair_referencias_cruzadas(texto),
                'valores': self.analisador.extrair_valores_numericos(texto)
            }
        }

    def _dividir_texto(self, e: ElementoRegulatorio) -> List[str]:
        sentencas = re.split(r'(?<=[.;!?])\s+', e.texto)
        chunks = []
        atual = ""
        for s in sentencas:
            if len(atual) + len(s) > self.tamanho_max_chunk:
                if atual: chunks.append(atual)
                atual = s
            else: atual += (' ' if atual else '') + s
        if atual: chunks.append(atual)
        return chunks

# ================================================================================
# 4. HANDLERS E UTILIT√ÅRIOS DE EXECU√á√ÉO
# ================================================================================

async def handle_process(request, env):
    """
    Handler principal para processamento de documentos no Cloudflare Worker.
    Substitui a l√≥gica anterior e utiliza pdfplumber via extract_text_from_pdf local.
    """
    try:
        from js import JSON
        from pyodide.ffi import to_js
        from utils.vectorize import process_and_vectorize_chunks

        body = (await request.json()).to_py()
        document_id = body.get("document_id")
        start_chunk = body.get("start_chunk", 0)
        limit_chunks = body.get("limit_chunks", 50)

        if not document_id:
            return Response.new(json.dumps({"error": "document_id is required"}), to_js({"status": 400}))

        # 1. Buscar metadados no D1
        doc_result = (await env.agems_rag_db.prepare("SELECT * FROM documents WHERE id = ?").bind(document_id).first()).to_py()
        if not doc_result:
            return Response.new(json.dumps({"error": "Document not found"}), to_js({"status": 404}))

        # 2. Extrair texto do PDF (100 p√°ginas por vez)
        full_text, total_pages = await extract_text_from_pdf(env, doc_result["r2_key"], limit_pages=100)
        
        # 3. Executar Chunking
        chunker = ChunkerRegulatorio(tamanho_max_chunk=1000)
        chunks = chunker.criar_chunks(full_text)
        
        # 4. Vetorizar lote
        processed = await process_and_vectorize_chunks(env, document_id, doc_result, chunks, start_chunk, limit_chunks)
        
        total = len(chunks)
        current = start_chunk + processed
        finished = current >= total
        
        # Atualizar D1
        status = 'processed' if finished else 'processing'
        await env.agems_rag_db.prepare("UPDATE documents SET status = ?, chunk_count = ? WHERE id = ?").bind(status, current, document_id).run()

        return Response.new(json.dumps({
            "success": True, "total_processed": current, "total_chunks": total, "is_finished": finished, "chunks_in_batch": processed
        }), JSON.parse(json.dumps({"headers": {"Content-Type": "application/json"}})))

    except Exception as e:
        import traceback
        return Response.new(json.dumps({"error": str(e), "trace": traceback.format_exc()}), to_js({"status": 500}))

async def handle_add_chunks(request, env):
    """Handler para ingest√£o externa (ingest.py)"""
    try:
        from js import JSON
        from utils.vectorize import process_and_vectorize_chunks
        body = (await request.json()).to_py()
        document_id = request.url.split("/documents/")[1].split("/chunks")[0]
        chunks_data = body.get("chunks", [])
        
        doc_db = (await env.agems_rag_db.prepare("SELECT * FROM documents WHERE id = ?").bind(document_id).first()).to_py()
        meta = {**body.get("metadata", {}), **doc_db}
        processed = await process_and_vectorize_chunks(env, document_id, meta, chunks_data)
        
        await env.agems_rag_db.prepare("UPDATE documents SET chunk_count = chunk_count + ? WHERE id = ?").bind(processed, document_id).run()
        return Response.new(json.dumps({"success": True, "processed": processed}), to_js({"headers": {"Content-Type": "application/json"}}))
    except Exception as e:
        return Response.new(json.dumps({"error": str(e)}), to_js({"status": 500}))

# ================================================================================
# 5. MODO LOCAL E TESTES
# ================================================================================

def carregar_pdf_local(path: str) -> str:
    with pdfplumber.open(path) as pdf:
        return "\n".join([f"[[PAGINA:{i+1}]]\n{p.extract_text(layout=True) or ''}" for i, p in enumerate(pdf.pages)])

def exportar_txt(chunks: List[Dict], path: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write("RELAT√ìRIO DE CHUNKS CONSOLIDADO\n" + "="*80 + "\n")
        for c in chunks:
            f.write(f"\nCHUNK {c['chunk_id']} | P√ÅG: {c['pagina']} | TIPO: {c['tipo']}\n")
            f.write(f"CONTEXTO: {c['contexto_hierarquico']}\n" + "-"*80 + f"\n{c['texto']}\n")

if __name__ == "__main__":
    # Teste local se executado diretamente
    pdf_local = r"c:/Users/rlazaro/Documents/Projetos_AGEMS/agems-rag-api/documentos_para_processar/REN_1000_ANEEL.pdf"
    if os.path.exists(pdf_local):
        print("üöÄ Processando localmente com PDFPLUMBER...")
        text = carregar_pdf_local(pdf_local)
        chunks = ChunkerRegulatorio().criar_chunks(text)
        print(f"‚úÖ {len(chunks)} chunks gerados.")
        exportar_txt(chunks, pdf_local.replace(".pdf", "_consolidado.txt"))