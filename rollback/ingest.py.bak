import sys
import os
import json
import requests
import pdfplumber
import time
from typing import List, Dict

# Adiciona o diretório 'src' ao path para importar as utilidades oficiais do projeto
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from handlers.chunks import ChunkerRegulatorio

# Configurações
WORKER_BASE_URL = "https://agems-rag-api.dgeagems.workers.dev"
FOLDER_PATH = "./documentos_para_processar"

def extract_text_locally(filepath):
    """ Extrai o texto do PDF localmente usando pdfplumber """
    texto_paginas = []
    with pdfplumber.open(filepath) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            t = page.extract_text(layout=True)
            if t:
                # Injeta marcador de página para o Chunker processar
                texto_paginas.append(f"[[PAGINA:{i}]]\n{t}")
    return "\n\n".join(texto_paginas)

def generate_embeddings_locally(chunks: List[Dict]):
    """
    Gera embeddings localmente usando a API da Cloudflare Workers AI.
    Adapta o formato do ChunkerRegulatorio para o formato do Vectorize.
    """
    from dotenv import load_dotenv
    load_dotenv()
    
    ACCOUNT_ID = os.getenv("CLOUDFLARE_ACCOUNT_ID")
    API_TOKEN = os.getenv("CLOUDFLARE_API_TOKEN")
    
    if not ACCOUNT_ID or not API_TOKEN:
        print("      ERRO: Credenciais da Cloudflare não encontradas!")
        return []
    
    url = f"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/baai/bge-m3"
    headers = {"Authorization": f"Bearer {API_TOKEN}", "Content-Type": "application/json"}
    
    print(f"      -> Gerando {len(chunks)} embeddings via Cloudflare AI API...")
    chunks_with_embeddings = []
    
    for i, c in enumerate(chunks):
        text = c.get("texto", "")
        if len(text.strip()) < 10: continue
        
        try:
            # Sanitizar
            clean_text = text.replace('\x00', '').strip()
            response = requests.post(url, headers=headers, json={"text": clean_text}, timeout=30)
            
            if response.status_code == 200:
                emb = response.json()["result"]["data"][0]
                
                # Formato final para o Worker handle_add_chunks
                chunks_with_embeddings.append({
                    "id": c.get("chunk_id", f"chunk-{i}"),
                    "values": emb,  # O vectorize.py espera 'values' para o embedding
                    "text": clean_text,
                    "metadata": {
                        "tipo": c.get("tipo"),
                        "numero": str(c.get("numero", "")),
                        "pagina": c.get("pagina"),
                        "contexto": c.get("contexto_hierarquico", ""),
                        **c.get("semantica", {})
                    }
                })
                
                if (i + 1) % 50 == 0:
                    print(f"         -> {i + 1}/{len(chunks)} embeddings gerados...")
            else:
                print(f"         ERRO chunk {i}: {response.status_code}")
            
            time.sleep(0.1) # Rate limit friendly
        except Exception as e:
            print(f"         Falha no chunk {i}: {e}")
            
    return chunks_with_embeddings

def ingest_documents():
    if not os.path.exists(FOLDER_PATH):
        print(f"Erro: Pasta '{FOLDER_PATH}' não encontrada.")
        return

    files = [f for f in os.listdir(FOLDER_PATH) if f.lower().endswith(".pdf")]
    if not files:
        print("Nenhum PDF encontrado.")
        return

    chunker = ChunkerRegulatorio(tamanho_max_chunk=1000)

    for filename in files:
        filepath = os.path.join(FOLDER_PATH, filename)
        print(f"\n--- Processando: {filename} ---")
        
        try:
            # 1. Extração
            text = extract_text_locally(filepath)
            
            # 2. Chunking
            chunks = chunker.criar_chunks(text)
            print(f"   -> {len(chunks)} chunks gerados.")
            
            # 3. Embeddings
            chunks_ready = generate_embeddings_locally(chunks)
            
            # 4. Enviar para o Worker
            # Extrair um ID de documento (por exemplo, nome do arquivo sanitizado)
            doc_id = filename.replace(".pdf", "").replace(" ", "_").lower()
            
            print(f"   -> Enviando para o Worker ({len(chunks_ready)} vetores)...")
            # Endpoint: POST /documents/{id}/chunks
            worker_url = f"{WORKER_BASE_URL}/documents/{doc_id}/chunks"
            
            # Lotes de 50 para não estourar o limite de payload
            batch_size = 50
            for i in range(0, len(chunks_ready), batch_size):
                batch = chunks_ready[i:i+batch_size]
                res = requests.post(worker_url, json={"chunks": batch}, timeout=60)
                if res.status_code == 200:
                    print(f"      Lote {i//batch_size + 1} enviado com sucesso.")
                else:
                    print(f"      ERRO no lote {i//batch_size + 1}: {res.text}")

        except Exception as e:
            print(f"   ERRO ao processar {filename}: {e}")

if __name__ == "__main__":
    ingest_documents()
